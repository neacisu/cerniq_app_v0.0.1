# ============================================================================
# Cerniq.app - Docker Compose Base Configuration
# ============================================================================
# Version: 1.0.2
# Last Updated: 2026-02-04
# References: ADR-0015, ADR-0022, ADR-0027
# ============================================================================
# IMPORTANT: Networks are created externally via `docker network create`
# to ensure proper isolation and persistence across compose restarts.
# See: docs/infrastructure/network-topology.md
# ============================================================================

name: cerniq

# ============================================================================
# Networks (External)
# ============================================================================
# Networks are pre-created on servers (staging/production) with:
#   docker network create --subnet X.X.X.X/24 [--internal] <name>
#
# Subnets (standardized under 172.29.0.0/16):
# - cerniq_public:  172.29.10.0/24 (external access via orchestrator Traefik)
# - cerniq_backend: 172.29.20.0/24 (internal: API + Workers)
# - cerniq_data:    172.29.30.0/24 (internal: PostgreSQL + Redis)
# ============================================================================

networks:
  cerniq_public:
    external: true
  cerniq_backend:
    external: true
  cerniq_data:
    external: true

# ============================================================================
# Volumes
# ============================================================================
# Persistent storage for databases and application data
# ============================================================================

volumes: {}
# ============================================================================
# X-Common Configurations (YAML Anchors)
# ============================================================================
# Reusable configuration blocks for services
# ============================================================================

x-common-labels: &common-labels
  com.cerniq.app: "cerniq"
  com.cerniq.environment: "${CERNIQ_ENV:-development}"
  com.cerniq.version: "${CERNIQ_VERSION:-0.0.0}"

x-logging: &default-logging
  driver: json-file
  options:
    max-size: "50m"
    max-file: "5"

# ============================================================================
# Secrets
# ============================================================================
# Secrets are sourced from OpenBao and materialized to files on the host.
# Default path is /opt/cerniq/secrets in deployments.
# ============================================================================

# ============================================================================
# Services
# ============================================================================
# E0-S2-PR02: PostgreSQL 18.1 + PgBouncer
# ============================================================================

services:

  # ==========================================================================
  # PgBouncer 1.25 - Connection Pooling
  # ==========================================================================
  # Reference: pgbouncer-connection-pooling.md, ADR-0004
  # Security: Password read from secret file at runtime (F0.8.1)
  # ==========================================================================
  pgbouncer:
    # NOTE: specific tags are not consistently resolvable from all environments;
    # use latest and pin to digest if reproducibility is required.
    image: edoburu/pgbouncer:latest
    container_name: cerniq-pgbouncer
    restart: unless-stopped
    # Security hardening - F0.8.1.T003
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    # NOTE: read_only disabled - pgbouncer image needs to write config files
    # and access Docker secrets. Security maintained via network isolation,
    # no-new-privileges, and cap_drop ALL.
    # Run as deploy user to read tmpfs-rendered secrets (LXC-safe).
    user: "1000:1000"
    volumes:
      # Config + auth are rendered by OpenBao Agent into tmpfs (preferred) or host path.
      - ${CERNIQ_RENDERED_SECRETS_DIR:-/opt/cerniq/runtime-secrets}/infra:/etc/pgbouncer:ro
    environment:
      # Expose env to template (used indirectly via agent to select DB names).
      CERNIQ_ENV: ${CERNIQ_ENV:-staging}
    entrypoint: ["/usr/bin/pgbouncer", "/etc/pgbouncer/pgbouncer.ini"]
    extra_hosts:
      - "ct107-postgres:10.0.1.107"
    networks:
      cerniq_data:
        ipv4_address: 172.29.30.11
      cerniq_backend:
        ipv4_address: 172.29.20.11
    # NO ports exposed - internal network only (security)
    healthcheck:
      # pg_isready exit codes: 0=accepting, 3=no attempt (still OK for PgBouncer)
      test: ["CMD", "sh", "-c", "pg_isready -h localhost -p 64033 >/dev/null 2>&1; rc=$$?; [ \"$$rc\" = \"0\" ] || [ \"$$rc\" = \"3\" ]"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.25'
    labels:
      <<: *common-labels
      com.cerniq.service: "pgbouncer"
      com.cerniq.port: "64033"
    logging:
      <<: *default-logging

  # ==========================================================================
  # OpenBao Agent - API Service Sidecar
  # ==========================================================================
  # Injects secrets into API service via template rendering
  # Auto-renews tokens and database credentials
  # ==========================================================================
  openbao-agent-api:
    image: quay.io/openbao/openbao:2.5.0
    container_name: cerniq-openbao-agent-api
    restart: unless-stopped
    # Run as deploy user (LXC-safe file permissions).
    user: "1000:1000"
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - SETGID
      - SETUID
    command: agent -config=/openbao/config/agent-api.hcl
    environment:
      CERNIQ_ENV: ${CERNIQ_ENV:-staging}
    volumes:
      # Agent configuration
      - ../config/openbao/agent-api.hcl:/openbao/config/agent-api.hcl:ro
      - ../config/openbao/templates:/openbao/templates:ro
      # Secrets output volume (shared with API service)
      - ${CERNIQ_RENDERED_SECRETS_DIR:-/opt/cerniq/runtime-secrets}/api:/secrets
      # AppRole credentials (populated by setup script)
      - ../../secrets/api_role_id:/openbao/config/role_id:ro
      - ../../secrets/api_secret_id:/openbao/config/secret_id:ro
    networks:
      - cerniq_backend
    healthcheck:
      # Check if secrets file exists (template rendered)
      test: ["CMD", "test", "-f", "/secrets/api.env"]
      interval: 10s
      timeout: 5s
      retries: 30
      start_period: 30s
    deploy:
      resources:
        limits:
          memory: 128M
          cpus: '0.25'
        reservations:
          memory: 64M
          cpus: '0.1'
    labels:
      <<: *common-labels
      com.cerniq.service: "openbao-agent-api"
    logging:
      <<: *default-logging

  # ==========================================================================
  # OpenBao Agent - Workers Service Sidecar
  # ==========================================================================
  # Injects secrets into Workers services via template rendering
  # Auto-renews tokens and database credentials
  # ==========================================================================
  openbao-agent-workers:
    image: quay.io/openbao/openbao:2.5.0
    container_name: cerniq-openbao-agent-workers
    restart: unless-stopped
    # Run as deploy user (LXC-safe file permissions).
    user: "1000:1000"
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - SETGID
      - SETUID
    command: agent -config=/openbao/config/agent-workers.hcl
    environment:
      CERNIQ_ENV: ${CERNIQ_ENV:-staging}
    volumes:
      # Agent configuration
      - ../config/openbao/agent-workers.hcl:/openbao/config/agent-workers.hcl:ro
      - ../config/openbao/templates:/openbao/templates:ro
      # Secrets output volume (shared with Workers services)
      - ${CERNIQ_RENDERED_SECRETS_DIR:-/opt/cerniq/runtime-secrets}/workers:/secrets
      # AppRole credentials (populated by setup script)
      - ../../secrets/workers_role_id:/openbao/config/role_id:ro
      - ../../secrets/workers_secret_id:/openbao/config/secret_id:ro
    networks:
      - cerniq_backend
    healthcheck:
      # Check if secrets file exists (template rendered)
      test: ["CMD", "test", "-f", "/secrets/workers.env"]
      interval: 10s
      timeout: 5s
      retries: 30
      start_period: 30s
    deploy:
      resources:
        limits:
          memory: 128M
          cpus: '0.25'
        reservations:
          memory: 64M
          cpus: '0.1'
    labels:
      <<: *common-labels
      com.cerniq.service: "openbao-agent-workers"
    logging:
      <<: *default-logging

  # ==========================================================================
  # OpenBao Agent - Infra Sidecar (PgBouncer auth/config)
  # ==========================================================================
  openbao-agent-infra:
    image: quay.io/openbao/openbao:2.5.0
    container_name: cerniq-openbao-agent-infra
    restart: unless-stopped
    user: "1000:1000"
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - SETGID
      - SETUID
    command: agent -config=/openbao/config/agent-infra.hcl
    environment:
      CERNIQ_ENV: ${CERNIQ_ENV:-staging}
    volumes:
      - ../config/openbao/agent-infra.hcl:/openbao/config/agent-infra.hcl:ro
      - ../config/openbao/templates:/openbao/templates:ro
      - ${CERNIQ_RENDERED_SECRETS_DIR:-/opt/cerniq/runtime-secrets}/infra:/secrets
      - ../../secrets/infra_role_id:/openbao/config/role_id:ro
      - ../../secrets/infra_secret_id:/openbao/config/secret_id:ro
    networks:
      - cerniq_backend
    healthcheck:
      test: ["CMD", "test", "-f", "/secrets/pgbouncer.ini"]
      interval: 10s
      timeout: 5s
      retries: 30
      start_period: 30s
    deploy:
      resources:
        limits:
          memory: 128M
          cpus: '0.25'
        reservations:
          memory: 64M
          cpus: '0.1'
    labels:
      <<: *common-labels
      com.cerniq.service: "openbao-agent-infra"
    logging:
      <<: *default-logging

  vector:
    image: timberio/vector:0.53.0-debian
    container_name: cerniq-vector
    restart: unless-stopped
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - ../config/vector/vector.toml:/etc/vector/vector.toml:ro
    command: ["--config", "/etc/vector/vector.toml"]
    environment:
      CERNIQ_ENV: ${CERNIQ_ENV:-staging}
    networks:
      - cerniq_backend
    labels:
      <<: *common-labels
      com.cerniq.service: "vector"
    logging:
      <<: *default-logging

  otel-collector:
    image: otel/opentelemetry-collector-contrib:0.145.0
    container_name: cerniq-otel-collector
    restart: unless-stopped
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    volumes:
      - ../config/otel/otel-collector.yaml:/etc/otelcol-contrib/config.yaml:ro
    command: ["--config=/etc/otelcol-contrib/config.yaml"]
    ports:
      - "64070:4317"
      - "64071:4318"
    networks:
      - cerniq_backend
    labels:
      <<: *common-labels
      com.cerniq.service: "otel-collector"
    logging:
      <<: *default-logging

  cadvisor:
    # NOTE: version tags are not consistently published on gcr.io; use latest.
    # If you need strict pinning, replace with an image digest after pull.
    image: gcr.io/cadvisor/cadvisor:latest
    container_name: cerniq-cadvisor
    restart: unless-stopped
    privileged: false
    security_opt:
      - no-new-privileges:true
    ports:
      - "64094:8080"
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker:/var/lib/docker:ro
    networks:
      - cerniq_backend
    labels:
      <<: *common-labels
      com.cerniq.service: "cadvisor"
    logging:
      <<: *default-logging