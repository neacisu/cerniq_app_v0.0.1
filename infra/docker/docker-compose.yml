# ============================================================================
# Cerniq.app - Docker Compose Base Configuration
# ============================================================================
# Version: 1.0.2
# Last Updated: 2026-02-04
# References: ADR-0015, ADR-0022, ADR-0027
# ============================================================================
# IMPORTANT: Networks are created externally via `docker network create`
# to ensure proper isolation and persistence across compose restarts.
# See: docs/infrastructure/network-topology.md
# ============================================================================

name: cerniq

# ============================================================================
# Networks (External)
# ============================================================================
# Networks are pre-created on servers (staging/production) with:
#   docker network create --subnet X.X.X.X/24 [--internal] <name>
#
# Subnets (standardized under 172.29.0.0/16):
# - cerniq_public:  172.29.10.0/24 (external access via orchestrator Traefik)
# - cerniq_backend: 172.29.20.0/24 (internal: API + Workers)
# - cerniq_data:    172.29.30.0/24 (internal: PostgreSQL + Redis)
# ============================================================================

networks:
  cerniq_public:
    external: true
  cerniq_backend:
    external: true
  cerniq_data:
    external: true

# ============================================================================
# Volumes
# ============================================================================
# Persistent storage for databases and application data
# ============================================================================

volumes:
  # Redis data persistence (RDB snapshots)
  redis_data:
    driver: local

  # OpenBao Agent secrets volumes
  api_secrets:
    driver: local

  workers_secrets:
    driver: local

# ============================================================================
# X-Common Configurations (YAML Anchors)
# ============================================================================
# Reusable configuration blocks for services
# ============================================================================

x-common-labels: &common-labels
  com.cerniq.app: "cerniq"
  com.cerniq.environment: "${CERNIQ_ENV:-development}"
  com.cerniq.version: "${CERNIQ_VERSION:-0.0.0}"

x-logging: &default-logging
  driver: json-file
  options:
    max-size: "50m"
    max-file: "5"

# Resource limits baseline (base/staging)

x-resource-redis: &resource-redis
  limits:
    memory: 4G
    cpus: '1'
  reservations:
    memory: 2G
    cpus: '0.5'

# ============================================================================
# Secrets
# ============================================================================
# Secrets are sourced from OpenBao and materialized to files on the host.
# Default path is /opt/cerniq/secrets in deployments.
# ============================================================================

# ============================================================================
# Services
# ============================================================================
# E0-S2-PR02: PostgreSQL 18.1 + PgBouncer
# ============================================================================

services:

  # ==========================================================================
  # PgBouncer 1.25 - Connection Pooling
  # ==========================================================================
  # Reference: pgbouncer-connection-pooling.md, ADR-0004
  # Security: Password read from secret file at runtime (F0.8.1)
  # ==========================================================================
  pgbouncer:
    image: edoburu/pgbouncer:1.25.1-p0
    container_name: cerniq-pgbouncer
    restart: unless-stopped
    # Security hardening - F0.8.1.T003
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    # NOTE: read_only disabled - pgbouncer image needs to write config files
    # and access Docker secrets. Security maintained via network isolation,
    # no-new-privileges, and cap_drop ALL.
    # Using volume mount instead of Docker secrets since compose non-swarm
    # doesn't support secret uid/gid settings
    volumes:
      - ${CERNIQ_SECRETS_DIR:-./secrets}/postgres_password.txt:/run/secrets/postgres_password:ro
    environment:
      # NOTE: DATABASE_URL built at runtime from secret (see command below)
      # Required by edoburu/pgbouncer entrypoint
      DB_HOST: 10.0.1.107
      DB_PORT: 5432
      DB_NAME: cerniq
      DB_USER: c3rn1q
      # DB_PASSWORD read from secret via command
      LISTEN_PORT: 64033
      POOL_MODE: transaction
      MAX_CLIENT_CONN: 1000
      DEFAULT_POOL_SIZE: 50
      MIN_POOL_SIZE: 10
      RESERVE_POOL_SIZE: 20
      RESERVE_POOL_TIMEOUT: 5
      AUTH_TYPE: scram-sha-256
      IGNORE_STARTUP_PARAMETERS: extra_float_digits
      SERVER_RESET_QUERY: DISCARD ALL
      # Logging
      LOG_CONNECTIONS: 1
      LOG_DISCONNECTIONS: 1
      VERBOSE: 0
    # Security: Read password from mounted file at runtime, not in env vars
    # Override entrypoint to export DB_PASSWORD before running original entrypoint
    entrypoint:
      - /bin/sh
      - -c
      - |
        export DB_PASSWORD="$$(cat /run/secrets/postgres_password)"
        exec /entrypoint.sh /usr/bin/pgbouncer /etc/pgbouncer/pgbouncer.ini
    networks:
      cerniq_data:
        ipv4_address: 172.29.30.11
      cerniq_backend:
        ipv4_address: 172.29.20.11
    # NO ports exposed - internal network only (security)
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -h localhost -p 64033 -U c3rn1q || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.25'
    labels:
      <<: *common-labels
      com.cerniq.service: "pgbouncer"
      com.cerniq.port: "64033"
    logging:
      <<: *default-logging

  # ==========================================================================
  # Redis 8.6.0 - BullMQ Job Queues & Cache
  # ==========================================================================
  # Reference: ADR-0006, etapa0-plan-implementare-complet-v2.md, etapa0-port-matrix.md
  # CRITICAL: maxmemory-policy MUST be noeviction for BullMQ
  # Port: 64039 (internal only, accessed via cerniq_data + cerniq_backend)
  # NOTE: Redis attached to BOTH networks - cerniq_data (primary) + cerniq_backend
  #       (for BullMQ workers access). This deviates from network-topology.md matrix
  #       which shows Redis only on cerniq_data, but is REQUIRED for worker access.
  # ==========================================================================
  redis:
    image: redis:8.6.0
    container_name: cerniq-redis
    restart: unless-stopped
    # Security hardening - F0.8.1.T003
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    # NOTE: user "999:999" removed - Docker Compose non-swarm doesn't support
    # secret uid/gid settings, so Redis runs as container default (redis user)
    # Security maintained via: no-new-privileges, cap_drop ALL, read_only, network isolation
    read_only: true
    tmpfs:
      - /tmp:size=100M
    command:
      - /bin/sh
      - -c
      - |
        redis-server \
          --port 64039 \
          --requirepass "$$(cat /run/secrets/redis_password)" \
          --maxmemory ${REDIS_MAXMEMORY:-4gb} \
          --maxmemory-policy noeviction \
          --appendonly yes \
          --appendfsync everysec \
          --aof-use-rdb-preamble yes \
          --notify-keyspace-events Ex \
          --lazyfree-lazy-eviction yes \
          --lazyfree-lazy-expire yes \
          --lazyfree-lazy-server-del yes \
          --activedefrag yes \
          --tcp-keepalive 300 \
          --timeout 0 \
          --databases 16
    volumes:
      - redis_data:/data
      - ${CERNIQ_SECRETS_DIR:-./secrets}/redis_password.txt:/run/secrets/redis_password:ro
    networks:
      cerniq_data:
        ipv4_address: 172.29.30.20
      cerniq_backend:
        ipv4_address: 172.29.20.20
    # NO ports exposed - internal network only (security)
    healthcheck:
      test: ["CMD-SHELL", "redis-cli -p 64039 -a \"$$(cat /run/secrets/redis_password)\" ping | grep -q PONG"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    deploy:
      resources:
        <<: *resource-redis
    labels:
      <<: *common-labels
      com.cerniq.service: "redis"
      com.cerniq.port: "64039"
    logging:
      <<: *default-logging
  # ==========================================================================
  # OpenBao Agent - API Service Sidecar
  # ==========================================================================
  # Injects secrets into API service via template rendering
  # Auto-renews tokens and database credentials
  # ==========================================================================
  openbao-agent-api:
    image: quay.io/openbao/openbao:2.5.0
    container_name: cerniq-openbao-agent-api
    restart: unless-stopped
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - SETGID
      - SETUID
    command: agent -config=/openbao/config/agent-api.hcl
    volumes:
      # Agent configuration
      - ../config/openbao/agent-api.hcl:/openbao/config/agent-api.hcl:ro
      - ../config/openbao/templates:/openbao/templates:ro
      # Secrets output volume (shared with API service)
      - api_secrets:/secrets
      # AppRole credentials (populated by setup script)
      - ../../secrets/api_role_id:/openbao/config/role_id:ro
      - ../../secrets/api_secret_id:/openbao/config/secret_id:ro
    networks:
      - cerniq_backend
    healthcheck:
      # Check if secrets file exists (template rendered)
      test: ["CMD", "test", "-f", "/secrets/api.env"]
      interval: 10s
      timeout: 5s
      retries: 30
      start_period: 30s
    deploy:
      resources:
        limits:
          memory: 128M
          cpus: '0.25'
        reservations:
          memory: 64M
          cpus: '0.1'
    labels:
      <<: *common-labels
      com.cerniq.service: "openbao-agent-api"
    logging:
      <<: *default-logging

  # ==========================================================================
  # OpenBao Agent - Workers Service Sidecar
  # ==========================================================================
  # Injects secrets into Workers services via template rendering
  # Auto-renews tokens and database credentials
  # ==========================================================================
  openbao-agent-workers:
    image: quay.io/openbao/openbao:2.5.0
    container_name: cerniq-openbao-agent-workers
    restart: unless-stopped
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - SETGID
      - SETUID
    command: agent -config=/openbao/config/agent-workers.hcl
    volumes:
      # Agent configuration
      - ../config/openbao/agent-workers.hcl:/openbao/config/agent-workers.hcl:ro
      - ../config/openbao/templates:/openbao/templates:ro
      # Secrets output volume (shared with Workers services)
      - workers_secrets:/secrets
      # AppRole credentials (populated by setup script)
      - ../../secrets/workers_role_id:/openbao/config/role_id:ro
      - ../../secrets/workers_secret_id:/openbao/config/secret_id:ro
    networks:
      - cerniq_backend
    healthcheck:
      # Check if secrets file exists (template rendered)
      test: ["CMD", "test", "-f", "/secrets/workers.env"]
      interval: 10s
      timeout: 5s
      retries: 30
      start_period: 30s
    deploy:
      resources:
        limits:
          memory: 128M
          cpus: '0.25'
        reservations:
          memory: 64M
          cpus: '0.1'
    labels:
      <<: *common-labels
      com.cerniq.service: "openbao-agent-workers"
    logging:
      <<: *default-logging

  vector:
    image: timberio/vector:0.53.0-debian
    container_name: cerniq-vector
    restart: unless-stopped
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - ../config/vector/vector.toml:/etc/vector/vector.toml:ro
    command: ["--config", "/etc/vector/vector.toml"]
    environment:
      CERNIQ_ENV: ${CERNIQ_ENV:-staging}
    networks:
      - cerniq_backend
    depends_on:
      - redis
    labels:
      <<: *common-labels
      com.cerniq.service: "vector"
    logging:
      <<: *default-logging

  otel-collector:
    image: otel/opentelemetry-collector-contrib:0.145.0
    container_name: cerniq-otel-collector
    restart: unless-stopped
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    volumes:
      - ../config/otel/otel-collector.yaml:/etc/otelcol-contrib/config.yaml:ro
    command: ["--config=/etc/otelcol-contrib/config.yaml"]
    ports:
      - "64070:4317"
      - "64071:4318"
    networks:
      - cerniq_backend
    labels:
      <<: *common-labels
      com.cerniq.service: "otel-collector"
    logging:
      <<: *default-logging

  cadvisor:
    image: gcr.io/cadvisor/cadvisor:v0.56.2
    container_name: cerniq-cadvisor
    restart: unless-stopped
    privileged: false
    security_opt:
      - no-new-privileges:true
    ports:
      - "64094:8080"
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker:/var/lib/docker:ro
    networks:
      - cerniq_backend
    labels:
      <<: *common-labels
      com.cerniq.service: "cadvisor"
    logging:
      <<: *default-logging